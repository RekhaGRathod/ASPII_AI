{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UdJmRYLWn_9",
        "outputId": "e03d6af0-11a4-4cbe-8a4f-1a29db3abd26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yt-dlp\n",
            "  Using cached yt_dlp-2025.6.9-py3-none-any.whl.metadata (174 kB)\n",
            "Using cached yt_dlp-2025.6.9-py3-none-any.whl (3.3 MB)\n",
            "Installing collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.6.9\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXMWd8jMWsAN",
        "outputId": "0772dfa6-f837-40af-80dd-6d205e3fbcbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://youtu.be/LuU9fm20JPU?feature=shared\n",
            "[youtube] LuU9fm20JPU: Downloading webpage\n",
            "[youtube] LuU9fm20JPU: Downloading tv client config\n",
            "[youtube] LuU9fm20JPU: Downloading tv player API JSON\n",
            "[youtube] LuU9fm20JPU: Downloading ios player API JSON\n",
            "[youtube] LuU9fm20JPU: Downloading m3u8 information\n",
            "[info] LuU9fm20JPU: Downloading 1 format(s): 137+251\n",
            "[download] Destination: interview_videos\\Complex Datasets with SQL (Data Scientist Mock Interview).f137.mp4\n",
            "\n",
            "[download]   0.0% of   78.33MiB at  614.46KiB/s ETA 02:11\n",
            "[download]   0.0% of   78.33MiB at    1.25MiB/s ETA 01:03\n",
            "[download]   0.0% of   78.33MiB at    2.42MiB/s ETA 00:32\n",
            "[download]   0.0% of   78.33MiB at    4.17MiB/s ETA 00:18\n",
            "[download]   0.0% of   78.33MiB at    2.22MiB/s ETA 00:35\n",
            "[download]   0.1% of   78.33MiB at    2.61MiB/s ETA 00:30\n",
            "[download]   0.2% of   78.33MiB at    3.46MiB/s ETA 00:22\n",
            "[download]   0.3% of   78.33MiB at    4.78MiB/s ETA 00:16\n",
            "[download]   0.6% of   78.33MiB at    7.36MiB/s ETA 00:10\n",
            "[download]   1.3% of   78.33MiB at   11.26MiB/s ETA 00:06\n",
            "[download]   2.6% of   78.33MiB at   15.19MiB/s ETA 00:05\n",
            "[download]   5.1% of   78.33MiB at   18.63MiB/s ETA 00:03\n",
            "[download]  10.2% of   78.33MiB at   19.31MiB/s ETA 00:03\n",
            "[download]  12.5% of   78.33MiB at   19.65MiB/s ETA 00:03\n",
            "[download]  12.5% of   78.33MiB at  Unknown B/s ETA Unknown\n",
            "[download]  12.5% of   78.33MiB at    1.75MiB/s ETA 00:39  \n",
            "[download]  12.6% of   78.33MiB at    3.37MiB/s ETA 00:20\n",
            "[download]  12.6% of   78.33MiB at    6.23MiB/s ETA 00:11\n",
            "[download]  12.6% of   78.33MiB at    2.77MiB/s ETA 00:24\n",
            "[download]  12.6% of   78.33MiB at    3.43MiB/s ETA 00:19\n",
            "[download]  12.7% of   78.33MiB at    4.55MiB/s ETA 00:15\n",
            "[download]  12.9% of   78.33MiB at    5.03MiB/s ETA 00:13\n",
            "[download]  13.2% of   78.33MiB at    7.15MiB/s ETA 00:09\n",
            "[download]  13.8% of   78.33MiB at   11.02MiB/s ETA 00:06\n",
            "[download]  15.1% of   78.33MiB at   15.08MiB/s ETA 00:04\n",
            "[download]  17.6% of   78.33MiB at   19.45MiB/s ETA 00:03\n",
            "[download]  22.8% of   78.33MiB at   24.67MiB/s ETA 00:02\n",
            "[download]  25.3% of   78.33MiB at   23.48MiB/s ETA 00:02\n",
            "[download]  25.3% of   78.33MiB at  583.43KiB/s ETA 01:43\n",
            "[download]  25.3% of   78.33MiB at    1.30MiB/s ETA 00:45\n",
            "[download]  25.3% of   78.33MiB at    2.60MiB/s ETA 00:22\n",
            "[download]  25.3% of   78.33MiB at    4.80MiB/s ETA 00:12\n",
            "[download]  25.3% of   78.33MiB at    2.28MiB/s ETA 00:25\n",
            "[download]  25.3% of   78.33MiB at    2.76MiB/s ETA 00:21\n",
            "[download]  25.4% of   78.33MiB at    2.91MiB/s ETA 00:20\n",
            "[download]  25.6% of   78.33MiB at    4.16MiB/s ETA 00:14\n",
            "[download]  25.9% of   78.33MiB at    6.27MiB/s ETA 00:09\n",
            "[download]  26.5% of   78.33MiB at    9.66MiB/s ETA 00:05\n",
            "[download]  27.8% of   78.33MiB at   14.04MiB/s ETA 00:04\n",
            "[download]  30.4% of   78.33MiB at   10.96MiB/s ETA 00:04\n",
            "[download]  35.5% of   78.33MiB at    5.33MiB/s ETA 00:09\n",
            "[download]  37.9% of   78.33MiB at    6.23MiB/s ETA 00:07\n",
            "[download]  37.9% of   78.33MiB at  761.77KiB/s ETA 01:05\n",
            "[download]  37.9% of   78.33MiB at    1.63MiB/s ETA 00:29\n",
            "[download]  37.9% of   78.33MiB at    3.26MiB/s ETA 00:14\n",
            "[download]  37.9% of   78.33MiB at    6.02MiB/s ETA 00:08\n",
            "[download]  37.9% of   78.33MiB at    2.49MiB/s ETA 00:19\n",
            "[download]  38.0% of   78.33MiB at    2.83MiB/s ETA 00:17\n",
            "[download]  38.0% of   78.33MiB at    3.57MiB/s ETA 00:13\n",
            "[download]  38.2% of   78.33MiB at    5.09MiB/s ETA 00:09\n",
            "[download]  38.5% of   78.33MiB at    7.69MiB/s ETA 00:06\n",
            "[download]  39.2% of   78.33MiB at   11.43MiB/s ETA 00:04\n",
            "[download]  40.4% of   78.33MiB at   15.92MiB/s ETA 00:02\n",
            "[download]  43.0% of   78.33MiB at   19.22MiB/s ETA 00:02\n",
            "[download]  48.1% of   78.33MiB at   20.47MiB/s ETA 00:01\n",
            "[download]  50.6% of   78.33MiB at   20.65MiB/s ETA 00:01\n",
            "[download]  50.6% of   78.33MiB at  650.99KiB/s ETA 01:01\n",
            "[download]  50.6% of   78.33MiB at    1.46MiB/s ETA 00:26\n",
            "[download]  50.6% of   78.33MiB at    2.97MiB/s ETA 00:13\n",
            "[download]  50.6% of   78.33MiB at    5.54MiB/s ETA 00:06\n",
            "[download]  50.6% of   78.33MiB at    2.42MiB/s ETA 00:15\n",
            "[download]  50.7% of   78.33MiB at    2.80MiB/s ETA 00:13\n",
            "[download]  50.7% of   78.33MiB at    3.66MiB/s ETA 00:10\n",
            "[download]  50.9% of   78.33MiB at    5.05MiB/s ETA 00:07\n",
            "[download]  51.2% of   78.33MiB at    8.00MiB/s ETA 00:04\n",
            "[download]  51.9% of   78.33MiB at   12.24MiB/s ETA 00:03\n",
            "[download]  53.1% of   78.33MiB at   16.50MiB/s ETA 00:02\n",
            "[download]  55.7% of   78.33MiB at   19.35MiB/s ETA 00:01\n",
            "[download]  60.8% of   78.33MiB at   18.55MiB/s ETA 00:01\n",
            "[download]  62.9% of   78.33MiB at    9.09MiB/s ETA 00:03\n",
            "[download]  62.9% of   78.33MiB at  655.77KiB/s ETA 00:45\n",
            "[download]  62.9% of   78.33MiB at    1.45MiB/s ETA 00:20\n",
            "[download]  62.9% of   78.33MiB at    2.96MiB/s ETA 00:09\n",
            "[download]  62.9% of   78.33MiB at    5.43MiB/s ETA 00:05\n",
            "[download]  62.9% of   78.33MiB at    2.74MiB/s ETA 00:10\n",
            "[download]  63.0% of   78.33MiB at    3.61MiB/s ETA 00:08\n",
            "[download]  63.1% of   78.33MiB at    4.71MiB/s ETA 00:06\n",
            "[download]  63.2% of   78.33MiB at    6.60MiB/s ETA 00:04\n",
            "[download]  63.5% of   78.33MiB at    9.25MiB/s ETA 00:03\n",
            "[download]  64.2% of   78.33MiB at   11.91MiB/s ETA 00:02\n",
            "[download]  65.4% of   78.33MiB at   16.02MiB/s ETA 00:01\n",
            "[download]  68.0% of   78.33MiB at   19.65MiB/s ETA 00:01\n",
            "[download]  73.1% of   78.33MiB at   23.87MiB/s ETA 00:00\n",
            "[download]  75.2% of   78.33MiB at   24.82MiB/s ETA 00:00\n",
            "[download]  75.2% of   78.33MiB at  Unknown B/s ETA Unknown\n",
            "[download]  75.2% of   78.33MiB at    2.09MiB/s ETA 00:09  \n",
            "[download]  75.2% of   78.33MiB at    3.97MiB/s ETA 00:04\n",
            "[download]  75.2% of   78.33MiB at    6.80MiB/s ETA 00:02\n",
            "[download]  75.2% of   78.33MiB at    1.72MiB/s ETA 00:11\n",
            "[download]  75.3% of   78.33MiB at    3.05MiB/s ETA 00:06\n",
            "[download]  75.4% of   78.33MiB at    3.24MiB/s ETA 00:05\n",
            "[download]  75.5% of   78.33MiB at    5.09MiB/s ETA 00:03\n",
            "[download]  75.8% of   78.33MiB at    7.91MiB/s ETA 00:02\n",
            "[download]  76.5% of   78.33MiB at   11.61MiB/s ETA 00:01\n",
            "[download]  77.7% of   78.33MiB at   15.73MiB/s ETA 00:01\n",
            "[download]  80.3% of   78.33MiB at   18.18MiB/s ETA 00:00\n",
            "[download]  85.4% of   78.33MiB at   19.80MiB/s ETA 00:00\n",
            "[download]  87.6% of   78.33MiB at   20.59MiB/s ETA 00:00\n",
            "[download]  87.6% of   78.33MiB at  611.24KiB/s ETA 00:16\n",
            "[download]  87.6% of   78.33MiB at    1.35MiB/s ETA 00:07\n",
            "[download]  87.6% of   78.33MiB at    2.71MiB/s ETA 00:03\n",
            "[download]  87.6% of   78.33MiB at    5.09MiB/s ETA 00:01\n",
            "[download]  87.6% of   78.33MiB at    2.86MiB/s ETA 00:03\n",
            "[download]  87.7% of   78.33MiB at    3.13MiB/s ETA 00:03\n",
            "[download]  87.8% of   78.33MiB at    4.29MiB/s ETA 00:02\n",
            "[download]  87.9% of   78.33MiB at    6.00MiB/s ETA 00:01\n",
            "[download]  88.2% of   78.33MiB at    9.12MiB/s ETA 00:01\n",
            "[download]  88.9% of   78.33MiB at   13.17MiB/s ETA 00:00\n",
            "[download]  90.1% of   78.33MiB at   13.82MiB/s ETA 00:00\n",
            "[download]  92.7% of   78.33MiB at   19.85MiB/s ETA 00:00\n",
            "[download]  97.8% of   78.33MiB at   21.18MiB/s ETA 00:00\n",
            "[download] 100.0% of   78.33MiB at   21.01MiB/s ETA 00:00\n",
            "[download] 100.0% of   78.33MiB at  Unknown B/s ETA Unknown\n",
            "[download] 100.0% of   78.33MiB at  Unknown B/s ETA Unknown\n",
            "[download] 100.0% of   78.33MiB at    5.36MiB/s ETA 00:00  \n",
            "[download] 100.0% of   78.33MiB at    9.11MiB/s ETA 00:00\n",
            "[download] 100.0% of   78.33MiB at    2.62MiB/s ETA 00:00\n",
            "[download] 100.0% of   78.33MiB at    2.61MiB/s ETA 00:00\n",
            "[download] 100% of   78.33MiB in 00:00:06 at 12.27MiB/s  \n",
            "[download] Destination: interview_videos\\Complex Datasets with SQL (Data Scientist Mock Interview).f251.webm\n",
            "\n",
            "[download]   0.0% of   12.87MiB at  Unknown B/s ETA Unknown\n",
            "[download]   0.0% of   12.87MiB at    2.42MiB/s ETA 00:05  \n",
            "[download]   0.1% of   12.87MiB at    4.45MiB/s ETA 00:02\n",
            "[download]   0.1% of   12.87MiB at    7.19MiB/s ETA 00:01\n",
            "[download]   0.2% of   12.87MiB at    2.89MiB/s ETA 00:04\n",
            "[download]   0.5% of   12.87MiB at    3.62MiB/s ETA 00:03\n",
            "[download]   1.0% of   12.87MiB at    3.93MiB/s ETA 00:03\n",
            "[download]   1.9% of   12.87MiB at    5.53MiB/s ETA 00:02\n",
            "[download]   3.9% of   12.87MiB at    6.37MiB/s ETA 00:01\n",
            "[download]   7.8% of   12.87MiB at    9.03MiB/s ETA 00:01\n",
            "[download]  15.5% of   12.87MiB at   12.45MiB/s ETA 00:00\n",
            "[download]  31.1% of   12.87MiB at   15.29MiB/s ETA 00:00\n",
            "[download]  62.1% of   12.87MiB at   17.71MiB/s ETA 00:00\n",
            "[download]  76.1% of   12.87MiB at   18.56MiB/s ETA 00:00\n",
            "[download]  76.1% of   12.87MiB at  781.94KiB/s ETA 00:04\n",
            "[download]  76.1% of   12.87MiB at    1.62MiB/s ETA 00:01\n",
            "[download]  76.2% of   12.87MiB at    3.19MiB/s ETA 00:00\n",
            "[download]  76.2% of   12.87MiB at    5.73MiB/s ETA 00:00\n",
            "[download]  76.3% of   12.87MiB at    2.84MiB/s ETA 00:01\n",
            "[download]  76.6% of   12.87MiB at    3.02MiB/s ETA 00:00\n",
            "[download]  77.1% of   12.87MiB at    4.12MiB/s ETA 00:00\n",
            "[download]  78.0% of   12.87MiB at    5.54MiB/s ETA 00:00\n",
            "[download]  80.0% of   12.87MiB at    1.24MiB/s ETA 00:02\n",
            "[download]  83.9% of   12.87MiB at    2.36MiB/s ETA 00:00\n",
            "[download]  91.6% of   12.87MiB at    4.18MiB/s ETA 00:00\n",
            "[download] 100.0% of   12.87MiB at    5.96MiB/s ETA 00:00\n",
            "[download] 100% of   12.87MiB in 00:00:01 at 8.95MiB/s   \n",
            "[Merger] Merging formats into \"interview_videos\\Complex Datasets with SQL (Data Scientist Mock Interview).mp4\"\n",
            "Deleting original file interview_videos\\Complex Datasets with SQL (Data Scientist Mock Interview).f137.mp4 (pass -k to keep)\n",
            "Deleting original file interview_videos\\Complex Datasets with SQL (Data Scientist Mock Interview).f251.webm (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "!yt-dlp -o \"interview_videos/%(title)s.%(ext)s\" -f bestvideo+bestaudio --merge-output-format mp4 \"https://youtu.be/LuU9fm20JPU?feature=shared\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting moviepy\n",
            "  Using cached moviepy-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\users\\rekharathod\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (5.2.1)\n",
            "Collecting imageio<3.0,>=2.5 (from moviepy)\n",
            "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
            "  Using cached imageio_ffmpeg-0.6.0-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
            "Collecting numpy>=1.25.0 (from moviepy)\n",
            "  Downloading numpy-2.3.0-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Collecting proglog<=1.0.0 (from moviepy)\n",
            "  Using cached proglog-0.1.12-py3-none-any.whl.metadata (794 bytes)\n",
            "Collecting python-dotenv>=0.10 (from moviepy)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pillow<12.0,>=9.2.0 (from moviepy)\n",
            "  Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
            "Collecting tqdm (from proglog<=1.0.0->moviepy)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\rekharathod\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->proglog<=1.0.0->moviepy) (0.4.6)\n",
            "Using cached moviepy-2.2.1-py3-none-any.whl (129 kB)\n",
            "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
            "Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl (2.7 MB)\n",
            "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.7/2.7 MB 18.9 MB/s eta 0:00:00\n",
            "Using cached proglog-0.1.12-py3-none-any.whl (6.3 kB)\n",
            "Using cached imageio_ffmpeg-0.6.0-py3-none-win_amd64.whl (31.2 MB)\n",
            "Downloading numpy-2.3.0-cp313-cp313-win_amd64.whl (12.7 MB)\n",
            "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
            "   --------------- ------------------------ 5.0/12.7 MB 24.5 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 6.6/12.7 MB 25.4 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 7.1/12.7 MB 10.8 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.5/12.7 MB 13.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.7/12.7 MB 13.2 MB/s eta 0:00:00\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, python-dotenv, pillow, numpy, imageio_ffmpeg, proglog, imageio, moviepy\n",
            "\n",
            "   ---------------------------------------- 0/8 [tqdm]\n",
            "   ---------- ----------------------------- 2/8 [pillow]\n",
            "   ---------- ----------------------------- 2/8 [pillow]\n",
            "   ---------- ----------------------------- 2/8 [pillow]\n",
            "   ---------- ----------------------------- 2/8 [pillow]\n",
            "   ---------- ----------------------------- 2/8 [pillow]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   --------------- ------------------------ 3/8 [numpy]\n",
            "   -------------------- ------------------- 4/8 [imageio_ffmpeg]\n",
            "   -------------------- ------------------- 4/8 [imageio_ffmpeg]\n",
            "   -------------------- ------------------- 4/8 [imageio_ffmpeg]\n",
            "   -------------------- ------------------- 4/8 [imageio_ffmpeg]\n",
            "   ------------------------- -------------- 5/8 [proglog]\n",
            "   ------------------------------ --------- 6/8 [imageio]\n",
            "   ------------------------------ --------- 6/8 [imageio]\n",
            "   ------------------------------ --------- 6/8 [imageio]\n",
            "   ----------------------------------- ---- 7/8 [moviepy]\n",
            "   ----------------------------------- ---- 7/8 [moviepy]\n",
            "   ---------------------------------------- 8/8 [moviepy]\n",
            "\n",
            "Successfully installed imageio-2.37.0 imageio_ffmpeg-0.6.0 moviepy-2.2.1 numpy-2.3.0 pillow-11.2.1 proglog-0.1.12 python-dotenv-1.1.0 tqdm-4.67.1\n"
          ]
        }
      ],
      "source": [
        "!pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/m-bain/whisperx.git\n",
            "  Cloning https://github.com/m-bain/whisperx.git to c:\\users\\rekharathod\\appdata\\local\\temp\\pip-req-build-9n8ow7ot\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  ERROR: Error [WinError 2] The system cannot find the file specified while executing command git version\n",
            "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ffmpeg-python\n",
            "  Using cached ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting future (from ffmpeg-python)\n",
            "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Using cached ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
            "Installing collected packages: future, ffmpeg-python\n",
            "\n",
            "   ---------------------------------------- 0/2 [future]\n",
            "   ---------------------------------------- 0/2 [future]\n",
            "   ---------------------------------------- 0/2 [future]\n",
            "   ---------------------------------------- 0/2 [future]\n",
            "   ---------------------------------------- 0/2 [future]\n",
            "   ---------------------------------------- 0/2 [future]\n",
            "   ---------------------------------------- 0/2 [future]\n",
            "   ---------------------------------------- 2/2 [ffmpeg-python]\n",
            "\n",
            "Successfully installed ffmpeg-python-0.2.0 future-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git\n",
        "!pip install ffmpeg-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Audio extracted: audio.wav\n"
          ]
        }
      ],
      "source": [
        "import ffmpeg\n",
        "\n",
        "input_video = r\"C:\\Users\\RekhaRathod\\OneDrive - sa.global\\Desktop\\candidate selection process\\interview_videos\\Complex Datasets with SQL (Data Scientist Mock Interview).mp4\"\n",
        "output_audio = \"audio.wav\"\n",
        "\n",
        "try:\n",
        "    (\n",
        "        ffmpeg\n",
        "        .input(input_video)\n",
        "        .output(output_audio, ac=1, ar=16000)\n",
        "        .run(capture_stdout=True, capture_stderr=True)\n",
        "    )\n",
        "    print(\"‚úÖ Audio extracted: audio.wav\")\n",
        "except ffmpeg.Error as e:\n",
        "    print(\"‚ùå FFmpeg error:\")\n",
        "    print(e.stderr.decode())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting assemblyai\n",
            "  Downloading assemblyai-0.41.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting httpx>=0.19.0 (from assemblyai)\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pydantic>=1.10.17 (from assemblyai)\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7 in c:\\users\\rekharathod\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from assemblyai) (4.14.0)\n",
            "Collecting websockets>=11.0 (from assemblyai)\n",
            "  Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
            "Collecting anyio (from httpx>=0.19.0->assemblyai)\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting certifi (from httpx>=0.19.0->assemblyai)\n",
            "  Downloading certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.19.0->assemblyai)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting idna (from httpx>=0.19.0->assemblyai)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.19.0->assemblyai)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=1.10.17->assemblyai)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic>=1.10.17->assemblyai)\n",
            "  Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic>=1.10.17->assemblyai)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx>=0.19.0->assemblyai)\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Downloading assemblyai-0.41.5-py3-none-any.whl (49 kB)\n",
            "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.0/2.0 MB 18.3 MB/s eta 0:00:00\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
            "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
            "Installing collected packages: websockets, typing-inspection, sniffio, pydantic-core, idna, h11, certifi, annotated-types, pydantic, httpcore, anyio, httpx, assemblyai\n",
            "\n",
            "   ----------------------------------------  0/13 [websockets]\n",
            "   ----------------------------------------  0/13 [websockets]\n",
            "   --------- ------------------------------  3/13 [pydantic-core]\n",
            "   ------------ ---------------------------  4/13 [idna]\n",
            "   ------------------------ ---------------  8/13 [pydantic]\n",
            "   ------------------------ ---------------  8/13 [pydantic]\n",
            "   ------------------------ ---------------  8/13 [pydantic]\n",
            "   ------------------------ ---------------  8/13 [pydantic]\n",
            "   ------------------------ ---------------  8/13 [pydantic]\n",
            "   --------------------------- ------------  9/13 [httpcore]\n",
            "   --------------------------- ------------  9/13 [httpcore]\n",
            "   ------------------------------ --------- 10/13 [anyio]\n",
            "   --------------------------------- ------ 11/13 [httpx]\n",
            "   --------------------------------- ------ 11/13 [httpx]\n",
            "   ---------------------------------------- 13/13 [assemblyai]\n",
            "\n",
            "Successfully installed annotated-types-0.7.0 anyio-4.9.0 assemblyai-0.41.5 certifi-2025.6.15 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 pydantic-2.11.7 pydantic-core-2.33.2 sniffio-1.3.1 typing-inspection-0.4.1 websockets-15.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install assemblyai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Interviewer: So for today's question, you can assume that you work as a data scientist for an E commerce company and for this company they have a database containing these three tables here. So there's a transactions table, a product table and a users table. So the transactions, each of them are uniquely identified by an id. Each transaction is associated with a specific user ID and you also have the quantity, the time it was created and the product id. So the product ID of that table links to the ID in the products table. And for each product you also have the name and the price of the product product. And lastly, you have the users table where the ID column links back to the user ID in the transactions table and you have the name and the sex for each user. All right, so let's get started with our first question then. The first question today is can you write a query that reports the number of users, the number of transactions placed and the total order amount per month in the year 2020? For those of you who aren't already familiar with Exponent, Exponent helps you get your dream tech career with our online courses, expert coaching, peer to peer, mock interviewing, platform and inter database. Check it out@tryexponent.com.\n",
            "Candidate: Sounds great. If you don't mind, I'd like to ask some clarification questions about the expected output and also about the data before I start to talk about logic and start querying, if that's okay.\n",
            "Interviewer: Yeah, absolutely. Please go ahead.\n",
            "Candidate: So yeah, looking at the table structure, it looks like ID is primary key for all these tables, transaction products and users tables. And it looks like in the transactions table we have something called user ID and I'm assuming that is going to map to the ID in the users table. And same goes with the product ID here. And I'm also assuming every product that is in the transaction table is covered in the product table. The way there are no gaps between these two tables. Are these assumptions looking good or anything you would like to add?\n",
            "Interviewer: Yeah, that all sounds right to me.\n",
            "Candidate: Cool. And in terms of expected output, as you outlined, this sounds like a monthly reporting where we'll be looking at number of users, number of transactions and also the total order amount. The way I think I'm going to be computing order amount is I see price column in the products table and I also see quantity in the transaction. So I'm actually assuming I can multiply these two to get the total order amount. For example, if a user is buying two product and the price is $10, the total order amount is going to be $20. So 2 times 10. Does that make sense?\n",
            "Interviewer: Yeah, that totally makes sense.\n",
            "Candidate: Yeah. So first thing I'd like to do is usually explore the table to see what kind of data I have. It'll be interesting to actually note down all the data types that we have in this table. One thing I can tell right off of the bat is it looks like we don't have to use user stable. So that's because we're not going to be reporting on name or gender at the moment. So I'm going to be using the first two table transactions and the product table. So let me do a quick querying. It looks like these tables have data, so let me outline the logic. Now what I'm going to do is I'm going to group all stats by month and I'm going to get month from the created at, which looks like a date time here. And then I'm going to use distinct count on the user to get the user account. Because it is possible that a user could have done two orders, but when it comes to the monthly report, we would only count them once because that's one user. Cool. Yeah. Anything you would like to add on the logic? Otherwise I'll get started with the query.\n",
            "Interviewer: No, I think that makes sense to me. Cool.\n",
            "Candidate: So I'm going to call this monthly reporting of KPIs.\n",
            "Interviewer: Right.\n",
            "Candidate: So I am. I'm actually commenting on the logic first. So I'm calling this query a monthly reporting KPI. And what I'm doing is I'm pulling each row in the transaction and I'm assuming each row in the transactions table is an order. And then as I said before, I'm doing price times quantity to get the order amount in terms of year. I'm only looking at 2020. So what I'm doing is I'm using date part function, SQL function to actually get the year and making sure it's actually 2020. So yeah, in terms of the summary, the user ID is going to give me number of customers. Transaction table is going to give me number of orders and then order amount is nothing but price times quantity and I group everything by month. The way we will get this monthly report. I'm going to quickly run this and see what output we are going to get. The output here is is monthly name which is essentially pointing to the number of month in a year. And then we have number of customers, number of orders and order amount. So I think this is what you are looking for.\n",
            "Interviewer: Okay, perfect. That sounds really great. Is there anything else that you would like to improve in this Query.\n",
            "Candidate: Yeah. So in terms of readability, one thing that I usually do in my big queries is I create something called cte. Common table expression, especially price, times quantity could be confusing for new users to read when they're reading the query. So for that, I may create cte. I don't think it will impact performance that much, but I still think that's a good way to write the query. The other thing I could do is I could order this by month the way it's in the right order. Fortunately, the table that is given to me is in the right order. So that's why we are seeing it's ordered correctly. But in some cases, let's say this table is huge and the dates are kind of all over the place. Ordering will give readability of the report.\n",
            "Interviewer: Okay, perfect. All right. So I'm curious then, like, what if you were to actually modify the query so that you could search over, so you could do the same query but over like multiple years, what would you have to do? Change?\n",
            "Candidate: Yeah, that's actually a great question. So here I'm actually filtering down only to 2020. So if I were to expand this, I'll need to do a couple of changes. One, I need to take this filter out. That way I'm looking at the entire table. I'm not restricting any particular year. The other thing that I need to think about is this month name. Right now I'm actually outputting the number. So as we start to add more years, what happens is it is possible that we will have orders for January of last year and January of this year. In my report, if we run this initial query, the first query, it will all be clubbed into one. So what I will do is I'll basically change this to reflect month name. So those are two changes I think I need to be making. Does that make sense? Anything you'd like to add? Nope.\n",
            "Interviewer: That looks good to me. Okay, so let's move on to the next question then. So the next question then is who actually ordered the most? Can you write a query that will output the name of that user who has the most orders?\n",
            "Candidate: Yeah, definitely. So now we need to use Users table. Let's say we want to output the name. So the name is actually in the users table. And so technically I can reuse, but I'm going to start from scratch here. It's cleaner. So I'm going to actually look at the users table, I'm going to join transactions table and Users table, and I'm going to use user ID from the transactions Table to connect with the ID in the users table.\n",
            "Interviewer: Okay, cool.\n",
            "Candidate: So in terms of logic, let me outline the logic with you so as to make sure you and I are on the same page. So I'm going to join the transactions table as I said. I'm going to sort the output by number of order and then I can limit to one. The way I'm only going to get the top row. It looks like you only or you're interested in capturing the top user. So I'm going to use a limit function. That's okay. Yep. So let me start coding here. Okay. I'm doing inner join on transactions and users and I'm grouping this entire query by user ID and name. And here's where I'm getting the number of orders. So essentially every row in the transactions table is considered an order. And then I'm actually sorting this in the descending order. And one final thing I will do is limit to one. So now if I execute. Cool. It looks like Ruksha Jarvis is the person who ordered the most. And this person ordered four orders.\n",
            "Interviewer: Okay, perfect. So I'm curious though, what if there happened to be more than one user who like has the most orders?\n",
            "Candidate: Yeah, that's a great question. Right now I'm using this limit function or limit condition to make the query simple. But let's say there are a couple of people who ordered most. In that case, I will start to use window function. There are window functions like row, number, rank. If I were to use those table and I can decide which one, what I'm going to do in terms of a tie scenario, do I want to output all the names or do I want to only output one name? Based on what I want to do, I can use either of these window functions.\n",
            "Interviewer: Okay, yeah. And is there any sort of like trade offs between using the limit statement versus the window functions?\n",
            "Candidate: Yeah, in terms of limit it is actually I would say it's not scalable. So it works for this query because I know what the data is because I've seen the data in the first query. But as if we were to expand this to a bigger table, that's usually the case with companies. Then window function is more appropriate. We can actually do partition by different columns. We can play around with the sorting, we can play around with how we are going to rank with window function. So window function offer more functionality for both scalability also for large database.\n",
            "Interviewer: Okay, yeah, that makes a lot of sense. Okay, so I think this is a fantastic place to pause. You did a really great job on this and I'd be really curious to hear from you. What do you think went well and what do you think you would change?\n",
            "Candidate: Yeah, so I think the queries themselves are great. I could have done a little bit of optimization if I had more time. I could have used a CTE and maybe thought about performance as a factor if we are going to scale this query. But in terms of the queries themselves, I feel confident that it answered the questions that you had asked. So yeah, I feel it went really well.\n",
            "Interviewer: Yeah, I totally agree with a lot of those points actually. I really in particular like that you were very careful about asking clarifying questions and verifying that your assumptions were correct before plunging into answering the questions. And I like the structure that you had of writing in comments, first of all, like what the logic for each sub part of the query that you're going to write was going to be. So that major query is really clear, especially for these queries that are longer and more complex. I think that's really helpful. And I really like that you were careful about the details too, like using distinct count to make sure that you didn't end up with duplicate rows. Yeah. And as for what might be improved, I do agree, like it might have been interesting to hear about some optimizations. So one thing that might be interesting to talk about is what if you were to run these queries? What if these are queries that you needed to run repeatedly all the time to generate automatically updating reports or something like that? Are there any optimizations that you can make to the actual tables themselves to optimize for these queries?\n",
            "Candidate: Yeah, so indexing is a common thing that we use in database to make queries faster. So what I would do if I were to design these tables is figure out what are primary keys, what are some other foreign keys and then I'll figure out how do we index these table. For example, in this database for all these three tables, it looks like the ID table is a primary key. And then there is also product ID which we use to join with other table. So there I would create an index. By creating index, this will actually run much faster and the queries will actually scale.\n",
            "Interviewer: Yeah, that's exactly what I was thinking. Could you tell me a little bit more about why indexing makes these queries more efficient?\n",
            "Candidate: Yeah, so the way indexing is going to work in the back end is it's going to help the SQL engine to actually pick the right data in a short time frame. So let's say we are actually joining these two user IDs then what? So let's say the user ID is one and it will basically look for one in both these tables. If we index SQL engine knows exactly where to go and grab that data because it knows the ordering is going to be in either ascending or descending. So it can basically go either start at the top or at the bottom and scan for where it's going to get the one. So by indexing, we're actually making this scanning process quicker basically.\n",
            "Interviewer: Yeah, it's kind of like it created a hash map from of the index values to the rows in the table itself, right?\n",
            "Candidate: Yeah, exactly. Yep, that's right.\n",
            "Interviewer: Yeah. Okay, perfect. So that's all I have for today. Thank you so much for joining us, Raj. And thank you everybody for watching. Good luck on your upcoming interviews. Bye everyone.\n",
            "Speaker 3: Thanks so much for watching. Don't forget to hit the like and subscribe buttons below to let us know that this video is valuable for you. And of course, check out hundreds more videos just like this@tryexponent.com thanks for watching and good luck on your upcoming interview.\n"
          ]
        }
      ],
      "source": [
        "import assemblyai as aai\n",
        "\n",
        "# üîê Set your API key\n",
        "aai.settings.api_key = \"c57f746912f54963b8c2d865aab671b7\"\n",
        "\n",
        "# Create a transcriber instance\n",
        "transcriber = aai.Transcriber()\n",
        "\n",
        "# Transcribe with speaker labels\n",
        "transcript = transcriber.transcribe(\"audio.wav\", config=aai.TranscriptionConfig(speaker_labels=True))\n",
        "\n",
        "# üîÅ Create dynamic speaker map based on first appearance\n",
        "speaker_map = {}\n",
        "custom_labels = [\"Interviewer\", \"Candidate\"]  # Add more if needed\n",
        "label_index = 0\n",
        "\n",
        "for utterance in transcript.utterances:\n",
        "    speaker = utterance.speaker\n",
        "    if speaker not in speaker_map:\n",
        "        if label_index < len(custom_labels):\n",
        "            speaker_map[speaker] = custom_labels[label_index]\n",
        "        else:\n",
        "            speaker_map[speaker] = f\"Speaker {label_index + 1}\"  # fallback\n",
        "        label_index += 1\n",
        "\n",
        "# ‚úÖ Display transcript with custom labels\n",
        "for utterance in transcript.utterances:\n",
        "    label = speaker_map[utterance.speaker]\n",
        "    print(f\"{label}: {utterance.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Transcript saved to labeled_transcript.txt\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Save transcript to a text file\n",
        "with open(\"labeled_transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for utterance in transcript.utterances:\n",
        "        label = speaker_map[utterance.speaker]\n",
        "        f.write(f\"{label}: {utterance.text}\\n\")\n",
        "\n",
        "print(\"‚úÖ Transcript saved to labeled_transcript.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\rekharathod\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\rekharathod\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.5/1.5 MB 9.9 MB/s eta 0:00:00\n",
            "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Installing collected packages: regex, joblib, click, nltk\n",
            "\n",
            "   ---------------------------------------- 0/4 [regex]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   ---------- ----------------------------- 1/4 [joblib]\n",
            "   -------------------- ------------------- 2/4 [click]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ------------------------------ --------- 3/4 [nltk]\n",
            "   ---------------------------------------- 4/4 [nltk]\n",
            "\n",
            "Successfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-2.3.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\rekharathod\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rekharathod\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\rekharathod\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.0-cp313-cp313-win_amd64.whl (11.0 MB)\n",
            "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 1.8/11.0 MB 10.5 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 5.8/11.0 MB 14.6 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 8.9/11.0 MB 15.0 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.0/11.0 MB 13.6 MB/s eta 0:00:00\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "\n",
            "   ---------------------------------------- 0/3 [pytz]\n",
            "   ---------------------------------------- 0/3 [pytz]\n",
            "   ---------------------------------------- 0/3 [pytz]\n",
            "   ------------- -------------------------- 1/3 [tzdata]\n",
            "   ------------- -------------------------- 1/3 [tzdata]\n",
            "   ------------- -------------------------- 1/3 [tzdata]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   ---------------------------------------- 3/3 [pandas]\n",
            "\n",
            "Successfully installed pandas-2.3.0 pytz-2025.2 tzdata-2025.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\RekhaRathod\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Lists to store data\n",
        "qa_pairs = []\n",
        "clean_transcript = []\n",
        "\n",
        "# Helper\n",
        "current_question = None\n",
        "\n",
        "for utt in transcript.utterances:\n",
        "    label = speaker_map[utt.speaker]\n",
        "    text = utt.text.strip()\n",
        "\n",
        "    # Build clean transcript\n",
        "    if label == \"Candidate\":\n",
        "        sentiment = sia.polarity_scores(text)\n",
        "        score = sentiment[\"compound\"]\n",
        "        sentiment_label = (\n",
        "            \"Positive\" if score >= 0.05 else\n",
        "            \"Negative\" if score <= -0.05 else\n",
        "            \"Neutral\"\n",
        "        )\n",
        "    else:\n",
        "        sentiment_label = \"\"\n",
        "        score = \"\"\n",
        "\n",
        "    clean_transcript.append({\n",
        "        \"speaker\": label,\n",
        "        \"text\": text,\n",
        "        \"sentiment\": sentiment_label,\n",
        "        \"score\": score\n",
        "    })\n",
        "\n",
        "    # Build Q&A sentiment table\n",
        "    if label == \"Interviewer\":\n",
        "        current_question = text\n",
        "    elif label == \"Candidate\" and current_question:\n",
        "        qa_pairs.append({\n",
        "            \"question\": current_question,\n",
        "            \"answer\": text,\n",
        "            \"sentiment\": sentiment_label,\n",
        "            \"score\": score\n",
        "        })\n",
        "        current_question = None  # Reset for next pair\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'temp_id' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m df_transcript = pd.DataFrame(clean_transcript)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Save as CSV\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m qa_file = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtemp/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtemp_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_qa_sentiment.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m transcript_file = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtemp/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_clean_transcript.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m df_qa.to_csv(qa_file, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mNameError\u001b[39m: name 'temp_id' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_qa = pd.DataFrame(qa_pairs)\n",
        "df_transcript = pd.DataFrame(clean_transcript)\n",
        "\n",
        "# Save as CSV\n",
        "qa_file = f\"temp/{temp_id}_qa_sentiment.csv\"\n",
        "transcript_file = f\"temp/{temp_id}_clean_transcript.csv\"\n",
        "\n",
        "df_qa.to_csv(qa_file, index=False)\n",
        "df_transcript.to_csv(transcript_file, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            question  \\\n",
            "0  So for today's question, you can assume that y...   \n",
            "1                 Yeah, absolutely. Please go ahead.   \n",
            "2                 Yeah, that all sounds right to me.   \n",
            "3                    Yeah, that totally makes sense.   \n",
            "4          No, I think that makes sense to me. Cool.   \n",
            "\n",
            "                                              answer sentiment   score  \n",
            "0  Sounds great. If you don't mind, I'd like to a...  Positive  0.5980  \n",
            "1  So yeah, looking at the table structure, it lo...  Positive  0.8655  \n",
            "2  Cool. And in terms of expected output, as you ...  Positive  0.6597  \n",
            "3  Yeah. So first thing I'd like to do is usually...  Positive  0.9481  \n",
            "4  So I'm going to call this monthly reporting of...   Neutral  0.0000  \n",
            "       speaker                                               text sentiment  \\\n",
            "0  Interviewer  So for today's question, you can assume that y...             \n",
            "1    Candidate  Sounds great. If you don't mind, I'd like to a...  Positive   \n",
            "2  Interviewer                 Yeah, absolutely. Please go ahead.             \n",
            "3    Candidate  So yeah, looking at the table structure, it lo...  Positive   \n",
            "4  Interviewer                 Yeah, that all sounds right to me.             \n",
            "\n",
            "    score  \n",
            "0          \n",
            "1   0.598  \n",
            "2          \n",
            "3  0.8655  \n",
            "4          \n"
          ]
        }
      ],
      "source": [
        "print(df_qa.head())\n",
        "print(df_transcript.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\RekhaRathod\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "qa_pairs = []\n",
        "clean_transcript = []\n",
        "current_question = None\n",
        "\n",
        "for utt in transcript.utterances:\n",
        "    role = speaker_map[utt.speaker]\n",
        "    clean_transcript.append({\"speaker\": role, \"text\": utt.text})\n",
        "\n",
        "    if role == \"Interviewer\":\n",
        "        current_question = utt.text\n",
        "    elif role == \"Candidate\" and current_question:\n",
        "        sentiment = sia.polarity_scores(utt.text)\n",
        "        score = sentiment[\"compound\"]\n",
        "        label = (\"Positive\" if score >= 0.05 else\n",
        "                 \"Negative\" if score <= -0.05 else\n",
        "                 \"Neutral\")\n",
        "\n",
        "        qa_pairs.append({\n",
        "            \"question\": current_question,\n",
        "            \"answer\": utt.text,\n",
        "            \"sentiment\": label,\n",
        "            \"score\": round(score, 4)\n",
        "        })\n",
        "        current_question = None  # Reset after matched Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "temp_id = str(uuid.uuid4())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_qa = pd.DataFrame(qa_pairs)\n",
        "df_transcript = pd.DataFrame(clean_transcript)\n",
        "\n",
        "qa_file = f\"temp/{temp_id}_qa_sentiment.csv\"\n",
        "transcript_file = f\"temp/{temp_id}_clean_transcript.csv\"\n",
        "\n",
        "df_qa.to_csv(qa_file, index=False)\n",
        "df_transcript.to_csv(transcript_file, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\RekhaRathod\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download(\"vader_lexicon\")\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Holds Q&A pairs and sentiment\n",
        "qa_pairs = []\n",
        "current_question = None\n",
        "\n",
        "for utt in transcript.utterances:\n",
        "    label = speaker_map[utt.speaker]\n",
        "\n",
        "    if label == \"Interviewer\":\n",
        "        current_question = utt.text  # Set current question\n",
        "    elif label == \"Candidate\" and current_question:\n",
        "        # Analyze sentiment for the answer\n",
        "        sentiment = sia.polarity_scores(utt.text)\n",
        "        score = sentiment[\"compound\"]\n",
        "        sentiment_label = (\"Positive\" if score >= 0.05 else\n",
        "                           \"Negative\" if score <= -0.05 else\n",
        "                           \"Neutral\")\n",
        "\n",
        "        qa_pairs.append({\n",
        "            \"question\": current_question,\n",
        "            \"answer\": utt.text,\n",
        "            \"score\": score,\n",
        "            \"sentiment\": sentiment_label\n",
        "        })\n",
        "\n",
        "        current_question = None  # Reset after matched pair\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Q1: So for today's question, you can assume that you work as a data scientist for an E commerce company and for this company they have a database containing these three tables here. So there's a transactions table, a product table and a users table. So the transactions, each of them are uniquely identified by an id. Each transaction is associated with a specific user ID and you also have the quantity, the time it was created and the product id. So the product ID of that table links to the ID in the products table. And for each product you also have the name and the price of the product product. And lastly, you have the users table where the ID column links back to the user ID in the transactions table and you have the name and the sex for each user. All right, so let's get started with our first question then. The first question today is can you write a query that reports the number of users, the number of transactions placed and the total order amount per month in the year 2020? For those of you who aren't already familiar with Exponent, Exponent helps you get your dream tech career with our online courses, expert coaching, peer to peer, mock interviewing, platform and inter database. Check it out@tryexponent.com.\n",
            "   A : Sounds great. If you don't mind, I'd like to ask some clarification questions about the expected output and also about the data before I start to talk about logic and start querying, if that's okay.\n",
            "   Sentiment: Positive (Score: 0.598)\n",
            "\n",
            "üîπ Q2: Yeah, absolutely. Please go ahead.\n",
            "   A : So yeah, looking at the table structure, it looks like ID is primary key for all these tables, transaction products and users tables. And it looks like in the transactions table we have something called user ID and I'm assuming that is going to map to the ID in the users table. And same goes with the product ID here. And I'm also assuming every product that is in the transaction table is covered in the product table. The way there are no gaps between these two tables. Are these assumptions looking good or anything you would like to add?\n",
            "   Sentiment: Positive (Score: 0.866)\n",
            "\n",
            "üîπ Q3: Yeah, that all sounds right to me.\n",
            "   A : Cool. And in terms of expected output, as you outlined, this sounds like a monthly reporting where we'll be looking at number of users, number of transactions and also the total order amount. The way I think I'm going to be computing order amount is I see price column in the products table and I also see quantity in the transaction. So I'm actually assuming I can multiply these two to get the total order amount. For example, if a user is buying two product and the price is $10, the total order amount is going to be $20. So 2 times 10. Does that make sense?\n",
            "   Sentiment: Positive (Score: 0.660)\n",
            "\n",
            "üîπ Q4: Yeah, that totally makes sense.\n",
            "   A : Yeah. So first thing I'd like to do is usually explore the table to see what kind of data I have. It'll be interesting to actually note down all the data types that we have in this table. One thing I can tell right off of the bat is it looks like we don't have to use user stable. So that's because we're not going to be reporting on name or gender at the moment. So I'm going to be using the first two table transactions and the product table. So let me do a quick querying. It looks like these tables have data, so let me outline the logic. Now what I'm going to do is I'm going to group all stats by month and I'm going to get month from the created at, which looks like a date time here. And then I'm going to use distinct count on the user to get the user account. Because it is possible that a user could have done two orders, but when it comes to the monthly report, we would only count them once because that's one user. Cool. Yeah. Anything you would like to add on the logic? Otherwise I'll get started with the query.\n",
            "   Sentiment: Positive (Score: 0.948)\n",
            "\n",
            "üîπ Q5: No, I think that makes sense to me. Cool.\n",
            "   A : So I'm going to call this monthly reporting of KPIs.\n",
            "   Sentiment: Neutral (Score: 0.000)\n",
            "\n",
            "üîπ Q6: Right.\n",
            "   A : So I am. I'm actually commenting on the logic first. So I'm calling this query a monthly reporting KPI. And what I'm doing is I'm pulling each row in the transaction and I'm assuming each row in the transactions table is an order. And then as I said before, I'm doing price times quantity to get the order amount in terms of year. I'm only looking at 2020. So what I'm doing is I'm using date part function, SQL function to actually get the year and making sure it's actually 2020. So yeah, in terms of the summary, the user ID is going to give me number of customers. Transaction table is going to give me number of orders and then order amount is nothing but price times quantity and I group everything by month. The way we will get this monthly report. I'm going to quickly run this and see what output we are going to get. The output here is is monthly name which is essentially pointing to the number of month in a year. And then we have number of customers, number of orders and order amount. So I think this is what you are looking for.\n",
            "   Sentiment: Positive (Score: 0.618)\n",
            "\n",
            "üîπ Q7: Okay, perfect. That sounds really great. Is there anything else that you would like to improve in this Query.\n",
            "   A : Yeah. So in terms of readability, one thing that I usually do in my big queries is I create something called cte. Common table expression, especially price, times quantity could be confusing for new users to read when they're reading the query. So for that, I may create cte. I don't think it will impact performance that much, but I still think that's a good way to write the query. The other thing I could do is I could order this by month the way it's in the right order. Fortunately, the table that is given to me is in the right order. So that's why we are seeing it's ordered correctly. But in some cases, let's say this table is huge and the dates are kind of all over the place. Ordering will give readability of the report.\n",
            "   Sentiment: Positive (Score: 0.842)\n",
            "\n",
            "üîπ Q8: Okay, perfect. All right. So I'm curious then, like, what if you were to actually modify the query so that you could search over, so you could do the same query but over like multiple years, what would you have to do? Change?\n",
            "   A : Yeah, that's actually a great question. So here I'm actually filtering down only to 2020. So if I were to expand this, I'll need to do a couple of changes. One, I need to take this filter out. That way I'm looking at the entire table. I'm not restricting any particular year. The other thing that I need to think about is this month name. Right now I'm actually outputting the number. So as we start to add more years, what happens is it is possible that we will have orders for January of last year and January of this year. In my report, if we run this initial query, the first query, it will all be clubbed into one. So what I will do is I'll basically change this to reflect month name. So those are two changes I think I need to be making. Does that make sense? Anything you'd like to add? Nope.\n",
            "   Sentiment: Positive (Score: 0.918)\n",
            "\n",
            "üîπ Q9: That looks good to me. Okay, so let's move on to the next question then. So the next question then is who actually ordered the most? Can you write a query that will output the name of that user who has the most orders?\n",
            "   A : Yeah, definitely. So now we need to use Users table. Let's say we want to output the name. So the name is actually in the users table. And so technically I can reuse, but I'm going to start from scratch here. It's cleaner. So I'm going to actually look at the users table, I'm going to join transactions table and Users table, and I'm going to use user ID from the transactions Table to connect with the ID in the users table.\n",
            "   Sentiment: Positive (Score: 0.754)\n",
            "\n",
            "üîπ Q10: Okay, cool.\n",
            "   A : So in terms of logic, let me outline the logic with you so as to make sure you and I are on the same page. So I'm going to join the transactions table as I said. I'm going to sort the output by number of order and then I can limit to one. The way I'm only going to get the top row. It looks like you only or you're interested in capturing the top user. So I'm going to use a limit function. That's okay. Yep. So let me start coding here. Okay. I'm doing inner join on transactions and users and I'm grouping this entire query by user ID and name. And here's where I'm getting the number of orders. So essentially every row in the transactions table is considered an order. And then I'm actually sorting this in the descending order. And one final thing I will do is limit to one. So now if I execute. Cool. It looks like Ruksha Jarvis is the person who ordered the most. And this person ordered four orders.\n",
            "   Sentiment: Positive (Score: 0.968)\n",
            "\n",
            "üîπ Q11: Okay, perfect. So I'm curious though, what if there happened to be more than one user who like has the most orders?\n",
            "   A : Yeah, that's a great question. Right now I'm using this limit function or limit condition to make the query simple. But let's say there are a couple of people who ordered most. In that case, I will start to use window function. There are window functions like row, number, rank. If I were to use those table and I can decide which one, what I'm going to do in terms of a tie scenario, do I want to output all the names or do I want to only output one name? Based on what I want to do, I can use either of these window functions.\n",
            "   Sentiment: Positive (Score: 0.848)\n",
            "\n",
            "üîπ Q12: Okay, yeah. And is there any sort of like trade offs between using the limit statement versus the window functions?\n",
            "   A : Yeah, in terms of limit it is actually I would say it's not scalable. So it works for this query because I know what the data is because I've seen the data in the first query. But as if we were to expand this to a bigger table, that's usually the case with companies. Then window function is more appropriate. We can actually do partition by different columns. We can play around with the sorting, we can play around with how we are going to rank with window function. So window function offer more functionality for both scalability also for large database.\n",
            "   Sentiment: Positive (Score: 0.867)\n",
            "\n",
            "üîπ Q13: Okay, yeah, that makes a lot of sense. Okay, so I think this is a fantastic place to pause. You did a really great job on this and I'd be really curious to hear from you. What do you think went well and what do you think you would change?\n",
            "   A : Yeah, so I think the queries themselves are great. I could have done a little bit of optimization if I had more time. I could have used a CTE and maybe thought about performance as a factor if we are going to scale this query. But in terms of the queries themselves, I feel confident that it answered the questions that you had asked. So yeah, I feel it went really well.\n",
            "   Sentiment: Positive (Score: 0.938)\n",
            "\n",
            "üîπ Q14: Yeah, I totally agree with a lot of those points actually. I really in particular like that you were very careful about asking clarifying questions and verifying that your assumptions were correct before plunging into answering the questions. And I like the structure that you had of writing in comments, first of all, like what the logic for each sub part of the query that you're going to write was going to be. So that major query is really clear, especially for these queries that are longer and more complex. I think that's really helpful. And I really like that you were careful about the details too, like using distinct count to make sure that you didn't end up with duplicate rows. Yeah. And as for what might be improved, I do agree, like it might have been interesting to hear about some optimizations. So one thing that might be interesting to talk about is what if you were to run these queries? What if these are queries that you needed to run repeatedly all the time to generate automatically updating reports or something like that? Are there any optimizations that you can make to the actual tables themselves to optimize for these queries?\n",
            "   A : Yeah, so indexing is a common thing that we use in database to make queries faster. So what I would do if I were to design these tables is figure out what are primary keys, what are some other foreign keys and then I'll figure out how do we index these table. For example, in this database for all these three tables, it looks like the ID table is a primary key. And then there is also product ID which we use to join with other table. So there I would create an index. By creating index, this will actually run much faster and the queries will actually scale.\n",
            "   Sentiment: Positive (Score: 0.858)\n",
            "\n",
            "üîπ Q15: Yeah, that's exactly what I was thinking. Could you tell me a little bit more about why indexing makes these queries more efficient?\n",
            "   A : Yeah, so the way indexing is going to work in the back end is it's going to help the SQL engine to actually pick the right data in a short time frame. So let's say we are actually joining these two user IDs then what? So let's say the user ID is one and it will basically look for one in both these tables. If we index SQL engine knows exactly where to go and grab that data because it knows the ordering is going to be in either ascending or descending. So it can basically go either start at the top or at the bottom and scan for where it's going to get the one. So by indexing, we're actually making this scanning process quicker basically.\n",
            "   Sentiment: Positive (Score: 0.691)\n",
            "\n",
            "üîπ Q16: Yeah, it's kind of like it created a hash map from of the index values to the rows in the table itself, right?\n",
            "   A : Yeah, exactly. Yep, that's right.\n",
            "   Sentiment: Positive (Score: 0.527)\n"
          ]
        }
      ],
      "source": [
        "for i, pair in enumerate(qa_pairs, 1):\n",
        "    print(f\"\\nüîπ Q{i}: {pair['question']}\")\n",
        "    print(f\"   A : {pair['answer']}\")\n",
        "    print(f\"   Sentiment: {pair['sentiment']} (Score: {pair['score']:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Sentiment Q&A pairs saved to qa_sentiment.txt\n"
          ]
        }
      ],
      "source": [
        "with open(\"qa_sentiment.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, pair in enumerate(qa_pairs, 1):\n",
        "        f.write(f\"Q{i}: {pair['question']}\\n\")\n",
        "        f.write(f\"A : {pair['answer']}\\n\")\n",
        "        f.write(f\"Sentiment: {pair['sentiment']} (Score: {pair['score']:.3f})\\n\\n\")\n",
        "\n",
        "print(\"‚úÖ Sentiment Q&A pairs saved to qa_sentiment.txt\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
